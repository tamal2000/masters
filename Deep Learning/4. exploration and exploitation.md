# Exploration vs Exploitation 

## Sequential decision making: 
- Agent solves sequential problem by interacting with the environment. 
- learning by trial and error: action and its consequences 
- Not supervised: reward based 
- Active: partial control over data 
- Online: maximizing performance during learning, not afterwards 

## Probability Theories: 
- __Stochastic/random variable:__ randomly determined outcome/sample. 
    - x:Î© â {x1,x2,....}, here
    - x is the random variable 
    - Î© is the event/outcome
    - {x1,x2,....} sample space  
- __Probability Distribution :__ 
    - Discrete set of outcome: P(X=x_k) = p_k [probability of random variable x in a given probability distribution]. __âˆ‘ p_k = 1__
    - Continuous outcomes: 
        - density function f(x) such that P( a â‰¤ x â‰¤ b ) = <sub>a</sub>âˆ«<sup>b</sup> f(x)â€¢dx or P(x<X <x +dx)  = f(x).dx and <sub>-inf</sub>âˆ«<sup>+inf</sup>f(x)dx = 1
- __Expectation__
    - E(X) = âˆ‘ x_k P(X=x_k) = âˆ‘ x_k p_k [discrete]
    - E(X) = âˆ« x f(x)dx [continuous]
    - Var(X) = âˆ‘(x_k - Î¼)<sup>2</sup>p_k [discrete]
    - Var(X) = âˆ« (x - Î¼ )<sup>2</sup>f(x)dx [continuous]
    - E(aX + bX) = aE(X) + bE(X)
    - Var(aX) = a<sup>2</sup> Var(X) 
- Independent Events: P(Aâ‹‚B) = P(A)P(B)
- Sample mean and variance: E(X_n) = Î¼ and Var(X_n) = ğœ<sup>2</sup>/n
- Conditional Probability: P(A|B) = P(Aâ‹‚B)/P(B)
- Independence: P(Aâ‹‚B) = P(A)P(B|A)= P(B)P(A|B) 
- Conditional Independent: P(Aâ‹‚B|C) = P(A|C)P(B|C)
- Baye's Rule: P(A|B) = P(B|A)P(A)/P(B)
- Law of total probability: P(A) = âˆ‘ P (Aâ‹‚B<sub>i</sub>) = âˆ‘ P(A|B<sub>i</sub>)P(B<sub>i</sub>)
- Monte Carlo: Let X1, X2... Xn be the samples from given distribution: 
    - E(X) = âˆ‘ x_k.P_k or âˆ«xf(x)dx  â‰ˆ 1/n âˆ‘ X_i or 1/n âˆ‘ Æ’(x_i)
- Kullback-Leibler Divergence: 
    - KL(p||q) = âˆ‘ p_k log (p_k/q_k) or âˆ« p(x) log(p(x)/q(x))dx 
    - or KL(p||q) â‰ˆ 1/nâˆ‘ log(p(x)/q(x))


## K-armed bandit Problem 
- K-arm bandit(slot machine) 
- each arm has random payoff 
- Target: maximize cumulative payoff over time. 
- Reward distribution: probability distribution of payoff for each action 

### Formalizing k-arm bandit 
- k action available 
- for action t, the agent gets reward R<sub>t</sub> â‰ˆ Æ’<sub>a_t</sub> 
- Finite_horizon:total reward over T actions: âˆ‘ R<sub>t</sub>
- Infinite-horizon: discounted total reward = âˆ‘<sup>inf</sup> ğ›¾<sup>t</sup> R<sub>t</sub>
- Discount factor ğ›¾ is the probability of the game continuing after each step 

## Exploration: 
Explore when agent take random steps to know more about the environment 

## Exploitation:
Agent takes previously known action and its payoff to maximize its payoff. taking greedy option. a* = arg max Qt(a)

### Balance exploration and exploitation
- Finite horizon: Exploration should be decrease as the horizon gets closer 
- Infinite horizon: 
    - ğ›¾ < 1: exploration should decrease as the agent's uncertainty about expects rewards goes down. 
    - ğ›¾ = 1: infinitely delayed splurge [????]


### Action-Value Methods 
- Expected value Qt(a) â‰ˆ E[R_t|A_t] = (âˆ‘ r_i)/k_a
- increament implimentation: 
    - t + 1 = Q(t+1)(a) = Q(a) + 1/(K_a+1)[r(t+1) - Qt(a)]
    - NewE <- OldE + LearningRate[NewData - OldE]
- Learning Rate: 1/k, 1/K<sup>2</sup>, 1/âˆšk 
- epsilon-greedy: the agent selects a random actions with probability ğœ–, and greedy action otherwise 
- SoftMax exploration: the agent chooses actions according to a Boltzmann distribution. p(a) = e<sup>Q(a)/r</sup> / âˆ‘ e<sup>Q(a')/r</sup> 
- Optimistic initialization: 
    - initializes action-values estimates higher than the largest possible reward. 
    - always selects the greedy action 
    - rewards are always disappointing, directing the agent to the least explored arms. 

- Upper confidence bounds:
    - optimism in the face of uncertainty 
    - goal: try to reduce uncertainty 
    - focus on most uncertain actions 
    - compute confidence intervals for each action 
    - always take action with highest upper bound. 
- UCB1-Algorithm: 
    - a*<sub>t+1</sub> = arg max (Qt(a) + c âˆš ((log t)/N(a)))
    - Do not select the arm which has performed the best so far
    - Select that could reasonably perform best in the future 
    - c is a tunable parameter 
    - every time action a is sampled N(a) in creases, hence Ut(a) decrease 
    - that means UCB of other increases  
    


##