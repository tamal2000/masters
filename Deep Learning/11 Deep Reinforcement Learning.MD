# Deep Reinforcement Learning 

## Topics
- Deep Q learning 
- Actor-Critic Methods 
- Model-Based RL

## Reinforce: 
1. ğœ â‰ˆ ğ›’(ğœ|ğœƒ), 
where ğœ = reward, ğœƒ = network parameters 
2. ğœƒ â† ğœƒ + ğ›‚âˆ‘<sub>t</sub><sup>t-1</sup>ğ›¾<sup>t</sup>G<sup>t</sup> ğœµ<sub>ğœ½</sub>ğ…<sub>ğœ½</sub>(a<sub>t</sub>|s<sub>t</sub>) 
where G<sub>t</sub>: discounted reward to-go, 
3. G<sub>t</sub>: âˆ‘<sub>t'</sub><sup>t-1</sup>ğ›¾<sup>t'-t</sup> 

# Q-Function: 
Q-function is known as state-action value function. q-function is expected discounted reward to-go after executing action a in state t. [after the action is executed, the Q-function expects the agent to follow the policy]
**Q<sup>ğ…</sup>(s,a) = ğ”¼<sub>ğ›’(ğœ|ğœ‹, s,a)</sub>[G]**

## Recursive Reward to go: 
- G = r' + ğ›¾ G'
- Discount reward to go is equal to reward in next state and discount reward to go in next state. 

## Optimal Q-Function 

* Q*(s,a) =  E<sub>p(s',r'|s,a)</sub> [r' + ğ›¾E<sub>ğœ‹<sup>*</sup>(a'|s')</sub>[Q* (s',a')]] = E<sub>p(s',r'|s,a)</sub> [r' + ğ›¾ max Q* (s',a')] 



## ATARI Q-Function Estimator 
- output layer is linear output layer 

## Bootstrap error 
- arg min(Q (s,a) - E[r' + ğ›¾ max Q(s',a')])<sup>2</sup>
- Q(s,a) is predict 
- E[r' + ğ›¾ max Q(s',a')] is target 
## Deep Q Learning
1. Choose action, a âŸµ arg max Q (s, a')
2. Execute action, s',r'â‰ˆ p(s',r'|s,a)
3. update params: 
    - Î¸ â† Î¸ - ğ›‚ ğœµ<sub>ğœƒ</sub>(Q(s,a) -r' - ğ›¾ max Q (s',a'))<sup>2<sup>
    - s â† s'

## Improve DQN
- Epsilon Greedy: Exploitation only 
- moving target so detach target from graph 

1. Choose action, a âŸµ arg max Q (s, a') or random action 
2. Execute action, s',r'â‰ˆ p(s',r'|s,a)
3. update params: 
    - Î¸ â† Î¸ - ğ›‚ ğœµ<sub>ğœƒ</sub>(Q(s,a) -r' - ğ›¾ max Q (s',a'))<sup>2<sup>
    - s â† s'

# Actor-Critic 
## Policy Gradient: 
Expected reward is equal to expectation with sum over time steps and reinforce actions based on ont discounted reward to go. 

# Monte Carlo rollout: 
have high variance. 

## Baseline 
- subtracting a value form discounted reward to go (b<sub>t</sub> baseline)
- expected value of baseline is 0 
- value function baseline: 
- difference: 
    - reinforce: agent select action based on policy and find the error to fix the policy by exploration and exploitation. 
    - baseline: reinforce + baseline: expected probability of the wining, how much better i did than i expected to do. increase of random reward wrt expected reward. 
- Training: 
    - target: bootstrapped expected reward to go 
    - predict: expected reward or reward? 
    - Step 1: select action 
    - step 2: update the baseline 
    - Step 3: update the policy 

