# Represent Language

## Chapter 3
- Name an advantage and disadvantage of staking RNNs. 
    - Advantage:
    - Disadvantage: Challenging to deal with long sentence. 
- Which problem can be addressed by bidirectional RNNs?
- State two problems that can occur with simple RNNs. 
- Which mechanism do GRUs and LSTMs introduce to simple RNNs and what is its role? 
- Why do transformers use self-attention?
- what is the motivation for using multiple attention heads?
- Why are positional embeddings needed in Transformers but not in RNNs?

## Blog 'The Illustrated BERT'
- BERT: pre-trained transformer. semi-supervised training on large amounts of text from books, wikipedia etc. Supervised training on a specific task with labeled dataset. BERT is basically a trained Transformer Encoder stack
- What does the input and output of BERT look like?
    - Input: Movie/Product review. Output: is the review positive or negative?
- Why are [CLS] and [SEP] tokens needed for in the input for the `BERT` model?
    - [CLS] token for reasons that will become apparent later on. CLS here stands for Classification.
    - 
- What are the two tasks the BERT is pre-trained on?
- What is the main difference between word2vec/GloVe embeddings and BERT/ELMo embeddings?
    - Embedding: need some form of numeric representation that models can use in their calculation. 
    - Word2Vec showed that we can use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships
    - it’s a great idea to use embeddings that were pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. So it became possible to download a list of words and their embeddings generated by pre-training with Word2Vec or GloVe.
    - GloVe representation, then the word “stick” would be represented by this vector no-matter what the context was.
    -  ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings.
    - 
- What happens in the progress of masking in every layer?
- What is meant by downstream task? 
    - downstream tasks is what the field calls those supervised-learning tasks that utilize a pre-trained model or component
## Representing words and sentences as Vector
- symbol representation 
    - individual sounds
    - syllables 
    - morphemes 
    - phonetic features 
- word as symbol: The relationship between signifiant and signifie is arbitrary and baed on conventions. 
- Lexical Resources: Lexical resources represent words by providing a definition and describing its relationships to other words. A rose is a flower, A flower is a plant.  
- One-hot vector: number of dimensions equals size of vocabulary. Relations between words are not represented. 

## Represent Language
1. Distributional hypothesis 
2. Language based models 
    a. RNNs as language models 
    b. Transformers (BERT) as language models 

### Usage based approach    
- Word meaning can be inferred from context. 
- The more context, the better our guess 
- Words are similar, if they occur in similar contexts 

### Word Embeddings
The network will learn to represent similar words with similar representation/vector/embeddings. 
- train a neural network that learns to 
     a. predict a word given the surrounding context 
     b. predict the surrounding context given a word 
- similar words are expected to be close to each other in vector space. 
- One vector per word but words can have multiple meanings 

### What matters
- Context matters; old train/old train the young 
- Order matters: Play the record, record the play

## Language Models
- Traditional task
    - next word prediction : find word `x` which maximizes the probability of the sequence. 
        - unigram model: P(dog bites X) = P(X|bites)x P(bites|dogs)xP(dog)
    - calculate probabilities of a sequence

## Recurring neural networks as language model 
- input: 
    - words are mapped into token ids which are mapped into vectors using weight matrix X
    - W is initialized randomly but fixed. 
- Output: 
    - Softmax to the output vector to obtain a probability distribution over all tokens in the vocabulary. 
- the basic architecture: calculating the hidden layer is recurring for the RNN models
- ELMO 
    - Two stacked LSTM layers 
    - backward language model which predicts the previous word based on following words. 
    - bidirectional LSTM: the hidden states of the two directions aer concatenated. 
- deep learning pre-conditions:
    - fast computer, big enough datasets, inital weights were good enough to a good solution
- language modeling as pre-training 
    - ony the raw text no annotations are required. 
    - next word prediction requires both semantic and syntactic knowledge. 

## Transformers as language models
- the basic architecture 
    - a crucial component of the transformer is attention
    - Long distance dependencies are a challenge for many NLP models 
    - Not all elements of sequence are equally important to remember
    - Selective Attention: 
- Attention: is a learned vector that was developed for encoder-decoder task. For each decoding step, the attention vector indicates which parts of the input representation are more relevant. 
- Attention in Machine Translation: Not all tokens in the input sentence are equally relevant for generating the correct output token. 
- Seq in Transformers: 
    - letting go of the sequence constraints makes it easier to parallelize computation within sentence. 
    - self attention model relationship between words in a sequence. each word in a sequence can attend to every other word in the same sequence. 
    - Each input is multiplied with three different weight vector: query, key, value. 
- Multi-Head Attention: 
    - attention depends on the prespective, If the netowk learns multiple attention head. it might learn to aten to different aspec of language. 
- Word order: The order of words can change the meaning. psitional encoding idicating the poition of the word. 
- BERT 
    - More pragmatic, Masked languge modeling. 
    - MLM: Masked 15% of the training data and predict the masked word based on the context The model can attend to any other word in the sentence. 
    - Next Sentence Prediction:
        - extract pairs of sentences A and B from the training data 
        Training objective doest B follow after A. 
        50% B follow A in corpus, 50% B is another random sentence from corpus. 
    - input: 
        - [CLS]: beginning of the first sentence, [SEP]: end of each sentence. 
        - Sentence embeddings indicating whether a token belongs to sentence A or sentence B. 
    - Token: BERT uses WordPiece tokenization. 
    - Output: BERTbse returns a different sentence representation for every layer which is a concatenation of the token representation in  this layer. 
        - A token is represented differently depending on the context in which it occurs. 
        - A token is represented differently in every layer. 
- feature extraction 



## Attention
- The `encoder` passes a lot more data to the `decoder`. Instead of passing the last hidden state of the encoding stage, the encoder passes all the statees to the decoder. 
- An attention decoder does an extra step before producing its output, In orrder to focus on the pars of the input that are relavant to this decoding time step, the   `decoder` does
    1. look at the set of encoder hidden states it receive, 
    2. give hidden state a score
    3. multipy each hidden state by its softmax score, 