# Exploration vs Exploitation 

## Sequential decision making: 
- Agent solves sequential problem by interacting with the environment. 
- learning by trial and error: action and its consequences 
- Not supervised: reward based 
- Active: partial control over data 
- Online: maximizing performance during learning, not afterwards 

## Probability Theories: 
- __Stochastic/random variable:__ randomly determined outcome/sample. 
    - x:Ω ➝ {x1,x2,....}, here
    - x is the random variable 
    - Ω is the event/outcome
    - {x1,x2,....} sample space  
- __Probability Distribution :__ 
    - Discrete set of outcome: P(X=x_k) = p_k [probability of random variable x in a given probability distribution]. __∑ p_k = 1__
    - Continuous outcomes: 
        - density function f(x) such that P( a ≤ x ≤ b ) = <sub>a</sub>∫<sup>b</sup> f(x)•dx or P(x<X <x +dx)  = f(x).dx and <sub>-inf</sub>∫<sup>+inf</sup>f(x)dx = 1
- __Expectation__
    - E(X) = ∑ x_k P(X=x_k) = ∑ x_k p_k [discrete]
    - E(X) = ∫ x f(x)dx [continuous]
    - Var(X) = ∑(x_k - μ)<sup>2</sup>p_k [discrete]
    - Var(X) = ∫ (x - μ )<sup>2</sup>f(x)dx [continuous]
    - E(aX + bX) = aE(X) + bE(X)
    - Var(aX) = a<sup>2</sup> Var(X) 
- Independent Events: P(A⋂B) = P(A)P(B)
- Sample mean and variance: E(X_n) = μ and Var(X_n) = 𝜎<sup>2</sup>/n
- Conditional Probability: P(A|B) = P(A⋂B)/P(B)
- Independence: P(A⋂B) = P(A)P(B|A)= P(B)P(A|B) 
- Conditional Independent: P(A⋂B|C) = P(A|C)P(B|C)
- Baye's Rule: P(A|B) = P(B|A)P(A)/P(B)
- Law of total probability: P(A) = ∑ P (A⋂B<sub>i</sub>) = ∑ P(A|B<sub>i</sub>)P(B<sub>i</sub>)
- Monte Carlo: Let X1, X2... Xn be the samples from given distribution: 
    - E(X) = ∑ x_k.P_k or ∫xf(x)dx  ≈ 1/n ∑ X_i or 1/n ∑ ƒ(x_i)
- Kullback-Leibler Divergence: 
    - KL(p||q) = ∑ p_k log (p_k/q_k) or ∫ p(x) log(p(x)/q(x))dx 
    - or KL(p||q) ≈ 1/n∑ log(p(x)/q(x))


## K-armed bandit Problem 
- K-arm bandit(slot machine) 
- each arm has random payoff 
- Target: maximize cumulative payoff over time. 
- Reward distribution: probability distribution of payoff for each action 

### Formalizing k-arm bandit 
- k action available 
- for action t, the agent gets reward R<sub>t</sub> ≈ ƒ<sub>a_t</sub> 
- Finite_horizon:total reward over T actions: ∑ R<sub>t</sub>
- Infinite-horizon: discounted total reward = ∑<sup>inf</sup> 𝛾<sup>t</sup> R<sub>t</sub>
- Discount factor 𝛾 is the probability of the game continuing after each step 

## Exploration: 
Explore when agent take random steps to know more about the environment 

## Exploitation:
Agent takes previously known action and its payoff to maximize its payoff. taking greedy option. a* = arg max Qt(a)

### Balance exploration and exploitation
- Finite horizon: Exploration should be decrease as the horizon gets closer 
- Infinite horizon: 
    - 𝛾 < 1: exploration should decrease as the agent's uncertainty about expects rewards goes down. 
    - 𝛾 = 1: infinitely delayed splurge [????]


### Action-Value Methods 
- Expected value Qt(a) ≈ E[R_t|A_t] = (∑ r_i)/k_a
- increament implimentation: 
    - t + 1 = Q(t+1)(a) = Q(a) + 1/(K_a+1)[r(t+1) - Qt(a)]
    - NewE <- OldE + LearningRate[NewData - OldE]
- Learning Rate: 1/k, 1/K<sup>2</sup>, 1/√k 
- epsilon-greedy: the agent selects a random actions with probability 𝜖, and greedy action otherwise 
- SoftMax exploration: the agent chooses actions according to a Boltzmann distribution. p(a) = e<sup>Q(a)/r</sup> / ∑ e<sup>Q(a')/r</sup> 
- Optimistic initialization: 
    - initializes action-values estimates higher than the largest possible reward. 
    - always selects the greedy action 
    - rewards are always disappointing, directing the agent to the least explored arms. 

- Upper confidence bounds:
    - optimism in the face of uncertainty 
    - goal: try to reduce uncertainty 
    - focus on most uncertain actions 
    - compute confidence intervals for each action 
    - always take action with highest upper bound. 
- UCB1-Algorithm: 
    - a*<sub>t+1</sub> = arg max (Qt(a) + c √ ((log t)/N(a)))
    - Do not select the arm which has performed the best so far
    - Select that could reasonably perform best in the future 
    - c is a tunable parameter 
    - every time action a is sampled N(a) in creases, hence Ut(a) decrease 
    - that means UCB of other increases  
    


##